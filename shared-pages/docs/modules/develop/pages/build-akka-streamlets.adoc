:page-partial:
:toc:
:toc-title: ON THIS PAGE
:toclevels: 2
:page-supergroup-scala-java: Language

include::partial$include.adoc[]

The following sections describe how you can create an Akka streamlet.
Akka streamlets are `Streamlet`{empty}s that offer Akka Streams as the user-facing API.

As mentioned in the xref:use-akka-streamlets.adoc[Using Akka Streamlets], an Akka streamlet is defined by the following features:

* It has a shape - we call it `StreamletShape` - that declares the inlets and outlets of the streamlet. Any Akka streamlet needs to define a concrete shape by using the APIs available for the `StreamletShape` class.
* It has a `StreamletLogic` that defines the business logic of the Akka streamlet.

In this tutorial, we'll build a simple Akka streamlet that accepts data records in an inlet and writes them to the console. 
It can be useful to gain insights in the data that arrives at its inlet.
Let's call the streamlet `ReportPrinter`.

== Extending from AkkaStreamlet

Let's start with building the `ReportPrinter` streamlet.
The first thing to do is to extend the `cloudflow.akkastream.AkkaStreamlet` abstract class, as shown below:

[.tabset] 
Scala::
+
--
[source,scala]
----
package com.example

import akka.stream.scaladsl.Sink

import cloudflow.streamlets._
import cloudflow.streamlets.avro._

import cloudflow.akkastream._
import cloudflow.akkastream.scaladsl._

object ReportPrinter extends AkkaStreamlet {
  // 1. TODO Create inlets and outlets
  // 2. TODO Define the shape of the streamlet
  // 3. TODO Override createLogic to provide StreamletLogic
}
----
--
Java::
+
--
[source,java]
----
package com.example;

import akka.NotUsed;
import akka.stream.*;
import akka.stream.javadsl.*;

import cloudflow.streamlets.*;
import cloudflow.streamlets.avro.*;
import cloudflow.akkastream.*;
import cloudflow.akkastream.javadsl.*;

public class ReportPrinter extends AkkaStreamlet {
  // 1. TODO Create inlets and outlets
  // 2. TODO Define the shape of the streamlet
  // 3. TODO Override createLogic to provide StreamletLogic
}
----
--

The code snippet above shows an object `ReportPrinter` that extends `AkkaStreamlet`.
We have annotated with a `TODO` the steps needed to complete the implementation, which we are going to do in the next few sections.

The next step is to implement inlets and outlets of the streamlet.

== Inlets and outlets

The streamlet that we are building in this tutorial has one inlet and no outlet.

[.tabset] 
Scala::
+
[source,scala]
----
package com.example

import akka.stream.scaladsl.Sink

import cloudflow.streamlets._
import cloudflow.streamlets.avro._

import cloudflow.akkastream._
import cloudflow.akkastream.scaladsl._

object ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  val inlet = AvroInlet[Report]("report-in")

  // 2. TODO Define the shape of the streamlet
  // 3. TODO Override createLogic to provide StreamletLogic
}
----

Java::
+
[source,java]
----
package com.example;

import akka.NotUsed;
import akka.stream.*;
import akka.stream.javadsl.*;

import cloudflow.streamlets.*;
import cloudflow.streamlets.avro.*;
import cloudflow.akkastream.*;
import cloudflow.akkastream.javadsl.*;

public class ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  AvroInlet<Report> inlet = AvroInlet.<Report>create("report-in", Report.class);

  // 2. TODO Define the shape of the streamlet
  // 3. TODO Override createLogic to provide StreamletLogic
}
----

Cloudflow supports Avro encoded processing of data. 
We make this explicit by defining the inlet as `AvroInlet`. 
`Report` is the type of the data objects that is accepted by this inlet.
This means that an inlet defined by `AvroInlet[Report]` will _only_ accept Avro encoded data that follows the schema defined for the class `Report`. 
The class `Report` is generated by Cloudflow during application build time from the Avro schema that the user supplies - this ensures that the data which the inlet accepts conforms to the schema that the user had supplied earlier.
As an example we have the following Avro schema for the `Report` class. 
It contains a summary of the attributes of products from an inventory:

[source,json]
----
{
  "namespace": "com.example",
  "type": "record",
  "name": "Report",
  "fields": [
    {
      "name": "id",
      "type": "string"
    },
    {
      "name": "name",
      "type": "string"
    },
    {
      "name": "description",
      "type": "string"
    },
    {
      "name": "keywords",
      "type": {
        "type": "array",
        "items": "string"
      }
    }
  ]
}
----

In the definition of the inlet, "report-in" is the name of the inlet. 
As a best practice, we recommend that you use a domain-specific name for the inlet to  indicate the nature of data that this inlet is supposed to accept.

[NOTE]
====
This streamlet does not have any outlet. But in general outlets are defined similarly. `val out = AvroOutlet[Report]("out").withPartitioner(report => report.name)` 
defines an outlet that writes Avro encoded data for the object of type `Report`. 
Here, "report-out" is the name that we give to the outlet. 
`withPartitioner` is used to set a partitioning function, which is used to partition the data to the underlying streaming system.
If we don't specify a partitioner, the default `RoundRobinPartitioner` is used.
====

== Streamlet shape

Lets now define the shape of `ReportPrinter` by using the APIs in `Cloudflow.streamlets.StreamletShape`:

[.tabset] 
Scala::
+
[source,scala]
----
package com.example

import akka.stream.scaladsl.Sink

import cloudflow.streamlets._
import cloudflow.streamlets.avro._

import cloudflow.akkastream._
import cloudflow.akkastream.scaladsl._

object ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  val inlet = AvroInlet[Report]("report-in")

  // 2. Define the shape of the streamlet
  val shape = StreamletShape.withInlets(inlet)

  // 3. TODO Override createLogic to provide StreamletLogic
}
----

Java::
+
[source,java]
----
package com.example;

import akka.NotUsed;
import akka.stream.*;
import akka.stream.javadsl.*;

import cloudflow.streamlets.*;
import cloudflow.streamlets.avro.*;
import cloudflow.akkastream.*;
import cloudflow.akkastream.javadsl.*;

public class ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  AvroInlet<Report> inlet = AvroInlet.<Report>create("report-in", Report.class);

  // 2. Define the shape of the streamlet
  public StreamletShape shape() {
   return StreamletShape.createWithInlets(inlet);
  }

  // 3. TODO Override createLogic to provide StreamletLogic
}
----

The above code specifies that this streamlet has a "one inlet, no outlet" shape by overriding the `shape` method with this specific configuration. 
In general, `StreamletShape` offers methods to define arbitrary shapes for any streamlet. 
For example, to define a streamlet with 2 inlets and 2 outlets, we could write `StreamletShape.withInlets(in0, in1).withOutlets(valid, invalid)`.

So far, we have defined the inlets and outlets and used them to declare the particular shape of this streamlet.
The next, and arguably the most important step, is to define the `StreamletLogic` that contains our business logic.

== Defining the `StreamletLogic`

The `StreamletLogic` class makes it possible for a user to specify domain logic. 
It is defined as an abstract class in `cloudflow.akkastream.StreamletLogic` and provides an abstract method `run()` where the user can define the desired business logic for the Akka Streamlet.

[NOTE]
====
If the streamlet needs to define local state required for processing logic, for example, a `Map` to resolve values or a `List` of the most recent events, it must be put inside the `StreamletLogic` class and _not_ as part of the outer `Streamlet` class.
The `Streamlet` class is used by Cloudflow for extraction of metadata.
Cloudflow instantiates `Streamlet`{empty}s when the blueprint is verified, which can have unwanted side effects.
====

For this next step, we need to override `createLogic` from `AkkaStreamlet` in our `ReportPrinter` object.
`createLogic` needs to return an instance of `StreamletLogic` which will do the processing of the incoming `report`{empty}s based on the requirements of `ReportPrinter` object.

[.tabset] 
Scala::
+
[source,scala]
----
package com.example

import akka.stream.scaladsl.Sink

import cloudflow.streamlets._
import cloudflow.streamlets.avro._

import cloudflow.akkastream._
import cloudflow.akkastream.scaladsl._

object ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  val inlet = AvroInlet[Report]("report-in")

  // 2. Define the shape of the streamlet
  val shape = StreamletShape.withInlets(inlet)

  // 3. Override createLogic to provide StreamletLogic
  def createLogic = new RunnableGraphStreamletLogic() {
    def format(report: Report) = s"${report.name}\n]n${report.description}"
    def runnableGraph =
      plainSource(inlet)
        .to(Sink.foreach(report â‡’ println(format(report))))
  }
}
----

Java::
+
[source,java]
----
package com.example;

import akka.NotUsed;
import akka.stream.*;
import akka.stream.javadsl.*;

import cloudflow.streamlets.*;
import cloudflow.streamlets.avro.*;
import cloudflow.akkastream.*;
import cloudflow.akkastream.javadsl.*;

public class ReportPrinter extends AkkaStreamlet {
  // 1. Create inlets and outlets
  AvroInlet<Report> inlet = AvroInlet.<Report>create("report-in", Report.class);

  // 2. Define the shape of the streamlet
  public StreamletShape shape() {
   return StreamletShape.createWithInlets(inlet);
  }

  // 3. Override createLogic to provide StreamletLogic
  public RunnableGraphStreamletLogic createLogic() {
    return new RunnableGraphStreamletLogic(getStreamletContext()) {
      public String format(Report report) {
        return report.getName() + "\n\n" +report.getDescription();
      }
      public RunnableGraph<NotUsed> createRunnableGraph() {
        return plainSource(inlet).to(Sink.foreach(report -> System.out.println(format(report))));
      }
    };
  }
}
----

In the above code, we override `createLogic` to supply the domain logic for the streamlet.

In this case, since we are implementing a printer streamlet for the console, all we need to do is read from the inlet that we defined earlier, `val inlet = AvroInlet[Report]("report-in")`, and do some processing on it to transform the record into a printable string.

We provide `RunnableGraphStreamletLogic` to facilitate the creation of the `StreamletLogic` when you only want to define a `RunnableGraph` to define the operation of this streamlet. 

It only requires you to define a `runnableGraph` method that specifies the graph to be run, as we have done in the above code. 
The `runnableGraph` method specifies that we create a `Source` from the `inlet` to read the reports and connect it to a `Sink` that prints out the reports.

Here are the steps that we take as part of the processing logic:

* Since we want to pretty print to the console, we define a helper `format` method to transform the input `Report` records into a formatted String.
* Every report read from the Source is printed by using `Sink.foreach`, which is part of the `akka.stream.scaladsl` package.

Note that the processing logic can be quite complex and we can maintain local ephimeral state as part of the implementation of `StreamletLogic`.

In summary, here are the steps for defining an Akka streamlet:

* Define the inlets and outlets
* Define the concrete shape using the inlets and outlets.
* Define the custom processing logic that reads data from inlets and writes data to outlets

=== Using `ReportPrinter` in the blueprint

An example of a blueprint using the `ReportPrinter` could look like this:

[source,hocon]
----
blueprint {
  streamlets {
    ingress = com.example.ReportIngress
    report-printer = com.example.ReportPrinter
  }

  connections {
    ingress.out = [report-printer.report-in]
  }
}

----

The omitted `ReportIngress` could, for instance, be another `AkkaStreamlet` that writes `Report`{empty}s to its outlet.

[[akka-at-least-at-most]]
== At-least-once or At-most-once processing

You can access the inlet and outlet streams through methods on the `StreamletLogic` that return Akka Streams `Source`{empty}s and `Sink`{empty}s.

So far, we've used the `plainSource` method to read from the inlet.
When the streamlet is started, it will only receive elements that arrive *after* it is started. 
The same applies when the streamlet is restarted for any reason. 
In short, the `plainSource` provides at-most-once message delivery semantics, as described in xref:cloudflow-streamlets.adoc#message-delivery-semantics[Message delivery semantics].

[NOTE]
====
The `plainSource` method optionally takes a `ResetPosition` argument. By default this is set to `Latest`, which means that it will only provide records as they arrive, starting at the latest offset known for every partition. 
If `ResetPosition` is set to `Earliest`, `plainSource` will start providing records from the earliest offset known for every partition, potentially re-reading *all received records* on *every* restart.
====

`StreamletLogic` also provides a `Source` that makes it possible to process records at-least-once. 
The `sourceWithOffsetContext` method creates a `Source`, where every record is automatically associated with a committable offset. 
That committable offset must be committed once you are done processing. 

There are several ways to commit offsets:

`SourceWithContext`:: used to pass through the committable offsets further downstream. 

`Flow`{empty}s and `Sink`{empty}s:: used to make sure that offsets are committed automatically.

All of these types move the committable offset along with the data but allow you to apply normal operators to the stream that treat the data transparently, as if it were without context. 

We explain these options in detail in the next sections.

=== Sources for inlets

As described, the `plainSource` methods returns an Akka Streams `Source`, which always starts reading elements as they arrive.

The `sourceWithOffsetContext` method returns a `akka.stream.scaladsl.SourceWithContext` for an inlet, the Java equivalent is `getSourceWithOffsetContext`, which returns a `akka.stream.javadsl.SourceWithContext`.

A `FlowWithContext`, which contains a subset of `Flow` operators, automatically propagates the context for every record, so that you don't have to worry about it when using it to process the stream. It can be easily attached to a `SourceWithContext` using the `via` method, similar to how a `Source` and `Flow` are connected.

[NOTE]
====
In the Scala API, we provide type aliases `cloudflow.akkastream.scaladsl.SourceWithOffsetContext[T]` and `cloudflow.akkastream.scaladsl.FlowWithOffsetContext[I, O]` which are short-hand for `akka.stream.scaladsl.SourceWithContext[T, CommittableOffset, NotUsed]`, `akka.stream.scaladsl.FlowWithContext[I, CommittableOffset, O, CommittableOffset, NotUsed]`, respectively.
====

=== Sinks for outlets

The `plainSink` method (Java API equivalent is `getPlainSink`) returns a sink to write to an outlet.

The `sinkWithOffsetContext` method (Java API equivalent is `getSinkWithOffsetContext`) returns a sink to write records to an outlet, *and* to commit the offsets associated with the records.

Records associated with offsets, written to this sink, are committed in batches. 
The `sinkWithOffsetContext` takes an optional `CommitterSettings` argument which controls how commits are batched. 
By default, the settings in `akka.kafka.committer` are used, which you can tune in your `application.conf`.

You can connect a `SourceWithContext` or a `FlowWithContext` to this type of sink with a `to` method, similar to how you would connect a `Source` or `Flow` to a `Sink`.
There is also a `sinkWithOffsetContext` method that takes no arguments.

It *only* commits the offsets associated with the records, the records themselves are ignored.

=== `FlowWithContext`

The `FlowWithContext` provides a constrained set of operators compared to the Akka Streams `Flow`. 
These subset of operators process records in-order.
It is used for your convenience. 
As a user you don't have to worry about how the offset per record is passed along.

=== Operators that turn `T` into `Seq[T]`

The `FlowWithContext` propagates the `CommittableOffset` per record that it operates on.
In the case of operators that create a `Seq[T]` of records for every record, like `grouped`, the sequencing operation is also applied to the context.
This means that the context is no longer `CommittableOffset`, but instead is turned into a `Seq[CommittableOffset]`.

If you transform the `Seq[T]` to some type that is written to an outlet, you have to
map the offsets with `mapContext` and select which offset you want to use, since now there is a `Seq[CommittableOffset]` where a single `CommittableOffset` is required. 
One way to do this is to use `flowWithContext.mapContext(_.last)` to select the last `CommittableOffset` associated with the grouped input records.

=== Converting between `FlowWithContext` and `Flow`

The Akka Streams `Flow` supports more operations than the `FlowWithContext` and allows for integrating with any kind of `Graph`, including custom `GraphStage`{empty}s.
The `FlowWithContext[In, CommittableOffset, Out, CommittableOffset, Mat]` can be converted to a `Flow[(In, CommittableOffset), (Out, CommittableOffset), Mat]`.
As you can see from the type signature, every element in the resulting `Flow` is a `Tuple2` of an element and its `CommittableOffset`. (In Java an `akka.japi.Pair` type is used instead of the Scala `Tuple2` type).
The `Flow[(In, CommittableOffset), (Out, CommittableOffset), Mat]` can be converted (back) to a `FlowWithContext[In, CommittableOffset, Out, CommittableOffset, Mat]`.

Being able to convert back and forth between `FlowWithContext` and `Flow` means that you can stop automatic context propagation, apply more advanced operations on the stream, and once you are finished, convert the `Flow` back into a `FlowWithContext`, as long as you pass along the elements with their contexts as tuples.

[NOTE]
====
If the order of elements is changed in custom operations on the Flow, it is likely that offsets will be advanced too early, which can result in data loss on Streamlet restart.
====

The same is true for the `SourceWithContext[CommittableOffset, Out, Mat]`, which can be turned into a `Source[(Out, CommittableOffset), Mat]`. The `endContextPropagation` method on `FlowWithContext` returns a `Flow`. `FlowWithContext.from` turns a `Flow` back into a `FlowWithContext`. The `endContextPropagation` method on `SourceWithContext` returns a `Source`. The `SourceWithContext.from` creates a `SourceWithContext` from a `Source`. In Java `SourceWithContext.fromPairs` is the equivalent for turning a `Source` into a `SourceWithContext`.

=== At-least-once ReportPrinter

The `plainSource` does not provide any tracking of where the streamlet left off, so if the streamlet is restarted it will print all elements from the earliest available data in the outlet it is connected to.

In this section, we're going to use the `sourceWithOffsetContext` to track offsets and commit them automatically with an `sinkWithOffsetContext`.

The `sourceWithOffsetContext` method provides a `SourceWithCommittableOffset[In]` for an inlet.
Internally, a `CommittableOffset` is paired with every element to keep track of the associated offset.
The `SourceWithContext` has many of the operators of a `Source`, it just automatically propagates the offsets.
In the code below, it is connected to an `sinkWithOffsetContext`, which automatically commits the offset associated with every record.

The code below shows how the `ReportPrinter` can be changed to use at-least-once semantics. All we need to do is change the streamlet logic:

[.tabset] 
Scala::
+
[source,scala]
----
def createLogic = new RunnableGraphStreamletLogic() {
  def format(report: Report) = s"${report.name}\n\n${report.description}"
  def runnableGraph =
    sourceWithOffsetContext(inlet)
      .map { report â‡’
        println(format(report))
        report
      }
      .to(sinkWithOffsetContext)
}
----

Java::
+
[source,java]
----
public RunnableGraph<NotUsed> createRunnableGraph() {
  return getSourceWithOffsetContext(inlet)
    .map(report -> {
        System.out.println(format(report));
        return report;
    })
    .asSource()
    .to(getSinkWithOffsetContext());
}
----

This completes the code for the `ReportPrinter`. The next step is to use it in a blueprint, as described in xref:cloudflow-streamlets.adoc#streamlets-blueprints[Composing streamlets using blueprints]. Then, you will want to xref:test-akka-streamlet.adoc[test your streamlets].
